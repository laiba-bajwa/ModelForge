{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLMRq0fi7KtjPHIGqfH2O4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laiba-bajwa/ModelForge/blob/main/Mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string #text and punctuation\n",
        "import nltk #tools\n",
        "from nltk.corpus import stopwords #stopwords\n",
        "from nltk.tokenize import word_tokenize # tokens\n",
        "from nltk.stem import PorterStemmer #Stemming\n",
        "from nltk.stem import WordNetLemmatizer #Lemmatization:\n",
        "from collections import Counter #to count\n",
        "from nltk import pos_tag,ne_chunk #part of speach and NER\n",
        "\n",
        "\n",
        "# Download stopwords data if not already available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')  # WordNet for lemmatization\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger for part-of-speech tagging\n",
        "nltk.download('maxent_ne_chunker_tab')#for NER\n",
        "nltk.download('words') # improve the accuracy of NER, as it includes a comprehensive list of common words.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmehhAHy7cBz",
        "outputId": "9e79037d-0faa-4df2-a5ac-e4e34576fb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq47Gnc_lQpw",
        "outputId": "d29cd3c7-a8de-40fc-974d-4eeedacde5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase Text: i have hope for the future and not hope in the past.\n",
            "No Punctuation: i have hope for the future and not hope in the past\n",
            "Cleaned Text: hope future hope past\n",
            "Tokens: ['hope', 'future', 'hope', 'past']\n",
            "Tokens: ['hope', 'future', 'hope', 'past']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "Corpus ka matlab hota hai ek bara collection ya body of text jo language\n",
        "ki analysis, research, aur machine learning models ke liye use hota hai.\n",
        "Ye text ka raw data hota hai jo kisay bhi language mein ho sakta hai,\n",
        "aur isay analyze karne ke liye text mining aur NLP\n",
        "tasks mein istemal kiya jata hai.\n",
        "\n",
        "Example: Wikipedia articles\n",
        "\"\"\"\n",
        "\n",
        "# Sample text (sentence containing \"hope\" or \"not hope\")\n",
        "text = \"I have hope for the future and not hope in the past.\"\n",
        "# Mapping words to binary labels (1 for 'hope', 0 for 'not hope')\n",
        "label_mapping = {\n",
        "    'hope': 1,  # Positive label\n",
        "    'not hope': 0  # Negative label\n",
        "}\n",
        "\n",
        "# Sample text\n",
        "text1 = \"hello!! my name is laiba and I am studying and running quickly to be better and 20 years old!\"\n",
        "\n",
        "# Convert the text to lowercase\n",
        "#df['message'] = df['message'].apply(lambda x: x.lower())  # Lowercase with columns\n",
        "text = text.lower()\n",
        "print(\"Lowercase Text:\", text)\n",
        "\n",
        "# Remove punctuation using string.punctuation\n",
        "nopunc = ''.join([c for c in text if c not in string.punctuation])\n",
        "print(\"No Punctuation:\", nopunc)\n",
        "\n",
        "# Load stopwords from nltk\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Remove stopwords from the text\n",
        "cleaned = ' '.join([word for word in nopunc.split() if word not in stop_words])\n",
        "print(\"Cleaned Text:\", cleaned)\n",
        "\n",
        "# Tokenization: Splitting the cleaned text into individual words (tokens)\n",
        "tokens = word_tokenize(cleaned)  # Tokenizes the cleaned text into a list of words\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "tokens = [word for word in tokens if word.isalpha()]#aghr (number remove krna ho yeh non-alpha charcter)\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "== Stemming:\n",
        "Stemming removes word endings to reduce words\n",
        " to their root form (e.g., \"running\" → \"run\").\n",
        "\n",
        "==Lemmatization:\n",
        "Lemmatization reduces words to their base or\n",
        " dictionary form (e.g., \"better\" → \"good\").\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "PorterStemmer ka object is liye banaya jata hai taake hum uske\n",
        "methods (jaise stem()) ko use kar sakein aur stemmer ko bar\n",
        "bar initialize na karna pade.\n",
        "\"\"\"\n",
        "# Create a stemmer object\n",
        "stemmer = PorterStemmer()# This initializes the PorterStemmer class\n",
        "stemmed_words=[stemmer.stem(word)for word in tokens]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Create a lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token\n",
        "lemmatized_words = [lemmatizer.lemmatize(word,pos='v') for word in tokens]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n",
        "\n",
        "#combine\n",
        "join_tokens=' '.join(lemmatized_words)\n",
        "print(\"Joined Text:\", join_tokens)\n",
        "\n",
        "#join_tokens is a string so we have to split it to count them\n",
        "#count tokens\n",
        "token_counter=Counter(join_tokens.split())\n",
        "print(token_counter.most_common(5))\n",
        "\n",
        "tagged = pos_tag(tokens)\n",
        "print(tagged)\n",
        "\n",
        "ner_tree=ne_chunk(tagged)\n",
        "print(\"Named Entities:\", ner_tree)\n",
        "\n",
        "\"\"\"\n",
        "POS Tags Example:\n",
        "\n",
        "PRP: Personal pronoun (I, you, etc.)\n",
        "\n",
        "VBP: Verb, non-3rd person singular present\n",
        "\n",
        "JJ: Adjective\n",
        "\n",
        "NNP: Proper noun, singular\n",
        "\n",
        "'NN' → Noun, singular\n",
        "\n",
        "'VBD' → Verb, past tense\n",
        "\n",
        "'NNP' → Proper noun, singular\n",
        "\n",
        "'CD' → Cardinal number\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "b2oVLm2w9vdx",
        "outputId": "199563c3-e6aa-4c11-cb66-b9463a6507ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['hope', 'futur', 'hope', 'past']\n",
            "Lemmatized Words: ['hope', 'future', 'hope', 'past']\n",
            "Joined Text: hope future hope past\n",
            "[('hope', 2), ('future', 1), ('past', 1)]\n",
            "[('hope', 'NN'), ('future', 'NN'), ('hope', 'NN'), ('past', 'IN')]\n",
            "Named Entities: (S hope/NN future/NN hope/NN past/IN)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPOS Tags Example:\\n\\nPRP: Personal pronoun (I, you, etc.)\\n\\nVBP: Verb, non-3rd person singular present\\n\\nJJ: Adjective\\n\\nNNP: Proper noun, singular\\n\\n'NN' → Noun, singular\\n\\n'VBD' → Verb, past tense\\n\\n'NNP' → Proper noun, singular\\n\\n'CD' → Cardinal number\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(cleaned)  # Tokenizes the cleaned text into a list of words\n",
        "print(\"Tokens:\", tokens)\n",
        "mapped_labels = [label_mapping.get(word, -1) for word in tokens]\n",
        "print(\"Mapped:\", mapped_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6uKLvkJ_l53",
        "outputId": "5eca9aab-2258-41b4-b9c0-cee889b221d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['hope', 'future', 'hope', 'past']\n",
            "Mapped: [1, -1, 1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"My name is Laiba. I study very hard, I will excel this time.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Define mapping for \"hope\" or \"not hope\" to positive or negative sentiment\n",
        "label_mapping = {\n",
        "    'hope': 1,    # Positive sentiment (hope)\n",
        "    'not hope': 0  # Negative sentiment (not hope)\n",
        "}\n",
        "\n",
        "# Apply the mapping to the tokens\n",
        "mapped_labels = [label_mapping.get(word, -1) for word in tokens]  # -1 for words not in the mapping\n",
        "\n",
        "# Print the mapped labels\n",
        "print(\"Mapped Labels:\", mapped_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K3LOEYgLXva",
        "outputId": "40805c24-fe2b-48e8-f507-1ea86f82a037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['my', 'name', 'is', 'laiba', '.', 'i', 'study', 'very', 'hard', ',', 'i', 'will', 'excel', 'this', 'time', '.']\n",
            "Mapped Labels: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
          ]
        }
      ]
    }
  ]
}